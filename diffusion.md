> VAE中的encoder改成固定的添加高斯噪声的过程

原理总结可以参考这篇博客：[https://shao.fun/blog/w/how-diffusion-models-work.html](https://shao.fun/blog/w/how-diffusion-models-work.html)
<a name="J5roI"></a>
## 原始论文
> [Deep unsupervised learning using nonequilibrium thermodynamics](http://proceedings.mlr.press/v37/sohl-dickstein15.html)

![](https://cdn.nlark.com/yuque/__latex/6865f01049936504d903dd8bf6ba1ae8.svg#card=math&code=x%5E0&id=fqpb5)为原始图片，![](https://cdn.nlark.com/yuque/__latex/34c7b563b30bde3c748139530686798e.svg#card=math&code=q&id=xceEt)表示前向的分布（先验），![](https://cdn.nlark.com/yuque/__latex/d4cd21d60552e207f237e82def9029b6.svg#card=math&code=p&id=Q9l6k)表示后向的分布（后验），![](https://cdn.nlark.com/yuque/__latex/6b1f9fa29dc931e0f770a7f2ae8e0674.svg#card=math&code=%5Cpi%28y%29&id=fHFg8)为期望的最终分布（tractable的）。
<a name="JYKns"></a>
### 前向过程
前向过程的条件概率：![](https://cdn.nlark.com/yuque/__latex/c32071b2750fbde0b004c24789436551.svg#card=math&code=q%28x%5Et%5Cmid%20x%5E%7Bt-1%7D%29%5Ctriangleq%20T_%5Cpi%28x%5Et%5Cmid%20x%5E%7Bt-1%7D%3B%5Cbeta_t%29&id=dSaBD)其中![](https://cdn.nlark.com/yuque/__latex/65ab8e5bfbba1d104086d7970722ee6c.svg#card=math&code=%5Cbeta_t&id=KyzBz)为diffusion rate（模型参数）<br />前向过程的轨迹：![](https://cdn.nlark.com/yuque/__latex/d19496d8e7651b80511485ad9778d43f.svg#card=math&code=q%28x%5E%7B0%3AT%7D%29%3Dq%28x%5E0%29%5Cprod_%7Bi%3D1%7D%5E%7BT%7Dq%28x%5Et%5Cmid%20x%5E%7Bt-1%7D%29&id=tVQ0I)
<a name="uQqqe"></a>
### 反向过程
反向过程的轨迹：![](https://cdn.nlark.com/yuque/__latex/78c6717efe17b376a3dc2e200243d41b.svg#card=math&code=p%28x%5E%7B0%3AT%7D%29%3Dp%28x%5ET%29%5Cprod_%7Bi%3D1%7D%5E%7BT%7Dp%28x%5E%7Bt-1%7D%5Cmid%20x%5E%7Bt%7D%29&id=gBKrA)，其中![](https://cdn.nlark.com/yuque/__latex/7c66773d52678186eb4ab2f277189297.svg#card=math&code=p%28x%5ET%29%3D%5Cpi%28x%5ET%29&id=Au2r9)<br />在极限情况下（![](https://cdn.nlark.com/yuque/__latex/948a39a3aa5535a7db74cd52c2117dc9.svg#card=math&code=%5Cbeta_t%5Cto%200%5E%2B%2CT%5Cto%2B%5Cinfty&id=o4tAF)），当q为高斯/二项分布，p也可以证明为高斯/二项分布
<a name="ITLk5"></a>
### loss的推导
> 只考虑高斯分布，文章讨论的二项分布暂不归纳，并且K我对文章中定义的取了负

1. 假定![](https://cdn.nlark.com/yuque/__latex/7bb61068819c7da4a8826b6c2c12635d.svg#card=math&code=p%28x%5E%7Bt-1%7D%5Cmid%20x%5E%7Bt%7D%29%3D%5Cmathcal%7BN%7D%28%5Cmu_%5Ctheta%28x%5Et%2Ct%29%2C%5CSigma_%5Ctheta%28x%5Et%2Ct%29%29&id=JNHFP)，模型学习的就是均值μ和方差Σ，本文考虑用MLP拟合。
2. 模型给数据生成的概率分布是![](https://cdn.nlark.com/yuque/__latex/baa2a2b87837b930e2ecee9ba69eee25.svg#card=math&code=p%28x%5E0%29%3D%5Cint%20p%28x%5E%7B0%3AT%7D%29d%7Bx%5E%7B1%3AT%7D%7D&id=ktwQG)（所有step的特征联合概率分布，对1:T step的特征积分）
3. 做如下转换，与前向概率相联系，使公式变的tractable：

![](https://cdn.nlark.com/yuque/__latex/85886ac30f2ab7384923ae6efe3af32c.svg#card=math&code=%5Cbegin%7Baligned%7D%0Ap%28x%5E0%29%0A%26%3D%0A%5Cint%20p%28x%5E%7B0%3AT%7D%29%5Cfrac%7Bq%28x%5E%7B1%3AT%7D%5Cmid%20x%5E%7B0%7D%29%7D%7Bq%28x%5E%7B1%3AT%7D%5Cmid%20x%5E%7B0%7D%29%7D%20dx%5E%7B1%3AT%7D%5C%5C%0A%26%3D%0A%5Cint%20%5Cfrac%7Bp%28x%5E%7B0%3AT%7D%29%7D%7Bq%28x%5E%7B1%3AT%7D%5Cmid%20x%5E%7B0%7D%29%7D%20%5C%3Bq%28x%5E%7B1%3AT%7D%5Cmid%20x%5E%7B0%7D%29%20dx%5E%7B1%3AT%7D%5C%5C%0A%26%3D%0A%5Cint%20p%28x%5ET%29%5Cprod_%7Bt%3D1%7D%5ET%5Cfrac%7Bp%28x%5E%7Bt-1%7D%5Cmid%20x%5Et%29%7D%7Bq%28x%5E%7Bt%7D%5Cmid%20x%5E%7Bt-1%7D%29%7D%20%5C%3Bq%28x%5E%7B1%3AT%7D%5Cmid%20x%5E%7B0%7D%29dx%5E%7B1%3AT%7D%5C%5C%0A%5Cend%7Baligned%7D&id=zyQNc)

4. 给定一个样本![](https://cdn.nlark.com/yuque/__latex/6865f01049936504d903dd8bf6ba1ae8.svg#card=math&code=x%5E0&id=T0nmy)通过多次前向，对所有轨迹求均值，就能得到积分![](https://cdn.nlark.com/yuque/__latex/2c05e17d6893f1b4ef70c17d2ce5eefb.svg#card=math&code=q%28x%5E%7B1%3AT%7D%5Cmid%20x%5E%7B0%7D%29dx%5E%7B1%3AT%7D&id=YeTha)的近似值（即蒙特卡洛方法），p用MLP计算，q用预定义的高斯分布计算。
5. 实操中只需要运行一次前向，因为在极限情况下，正反向轨迹可以被弄成相同，从而一次前向就是对积分完全准确的估计（这个性质我没完全懂）
6. 训练的目标是，最小化p与q的交叉熵：![](https://cdn.nlark.com/yuque/__latex/37b609c2fbd6ba75bfcf8570265c30a4.svg#card=math&code=L%3D%5Cint%20-%5Clog%20p%28x%5E0%29%5C%3Bq%28x%5E0%29dx%5E0&id=LQgxv)。也就是把所有样本用p求出来的概率负对数求均值
7. 使用Jensen不等式求L的下界（积分的log小于等于log的积分），我理解这一步避免了需要求积分的log（和的log是不可以拆分的）：

![](https://cdn.nlark.com/yuque/__latex/14875cc198ce404e9be96df91ddea694.svg#card=math&code=%5Cbegin%7Baligned%7D%0AL%26%3D%5Cint%20-%5Clog%20%5Cleft%28%5Cint%20p%28x%5ET%29%5Cprod_%7Bt%3D1%7D%5ET%5Cfrac%7Bp%28x%5E%7Bt-1%7D%5Cmid%20x%5Et%29%7D%7Bq%28x%5E%7Bt%7D%5Cmid%20x%5E%7Bt-1%7D%29%7D%20%5C%3Bq%28x%5E%7B1%3AT%7D%5Cmid%20x%5E%7B0%7D%29dx%5E%7B1%3AT%7D%5Cright%29%5C%3Bq%28x%5E0%29%20dx%5E0%20%5C%5C%0A%26%5Cgeq%0A%5Cint%20-%5Clog%20%5Cleft%28%20p%28x%5ET%29%5Cprod_%7Bt%3D1%7D%5ET%5Cfrac%7Bp%28x%5E%7Bt-1%7D%5Cmid%20x%5Et%29%7D%7Bq%28x%5E%7Bt%7D%5Cmid%20x%5E%7Bt-1%7D%29%7D%20%5Cright%29q%28x%5E%7B0%3AT%7D%29%20dx%5E%7B0%3AT%7D%5C%5C%0A%26%3D%0A%5Cint-%5Clog%20%5Cleft%28%20%5Cprod_%7Bt%3D1%7D%5ET%5Cfrac%7Bp%28x%5E%7Bt-1%7D%5Cmid%20x%5Et%29%7D%7Bq%28x%5E%7Bt%7D%5Cmid%20x%5E%7Bt-1%7D%29%7D%20%5Cright%29q%28x%5E%7B0%3AT%7D%29%20dx%5E%7B0%3AT%7D%0A%2B%0A%5Cint%20-%5Clog%20p%28x%5ET%29%20q%28x%5ET%29%20dx%5ET%0A%5Cend%7Baligned%7D&id=xFOUB)

8. 优化目标为![](https://cdn.nlark.com/yuque/__latex/23388e6d492bf4dc3c4df3aa3049e851.svg#card=math&code=K%3D%5Csum_%7Bt%3D1%7D%5ET%5Cint-%5Clog%20%5Cfrac%7Bp%28x%5E%7Bt-1%7D%5Cmid%20x%5Et%29%7D%7Bq%28x%5E%7Bt%7D%5Cmid%20x%5E%7Bt-1%7D%29%7D%20q%28x%5E%7B0%3AT%7D%29%20dx%5E%7B0%3AT%7D%2BH_p%28X%5ET%29&id=kFEmM)，最后一项是常数（![](https://cdn.nlark.com/yuque/__latex/5b686eda6a645d15a1f5db0dd81a6d8a.svg#card=math&code=x%5ET&id=Dsuc4)的分布为固定的正态分布）
9. 为了减小边界效应（原文：to avoid edge effects），文章假设![](https://cdn.nlark.com/yuque/__latex/16392572f9fbf2b14941f8b513ce252e.svg#card=math&code=p%28x%5E0%5Cmid%20x%5E1%29%3Dq%28x%5E1%5Cmid%20x%5E0%29&id=KpJuD)，从而![](https://cdn.nlark.com/yuque/__latex/01e6495577d94b7fb27965eb6f159ce6.svg#card=math&code=K%3D%5Csum_%7Bt%3D2%7D%5ET%5Cint-%5Clog%20%5Cfrac%7Bp%28x%5E%7Bt-1%7D%5Cmid%20x%5Et%29%7D%7Bq%28x%5E%7Bt%7D%5Cmid%20x%5E%7Bt-1%7D%29%7D%20q%28x%5E%7B0%3AT%7D%29%20dx%5E%7B0%3AT%7D%2BH_p%28X%5ET%29&id=bXgra)。为什么要这么做我没有理解，猜测是![](https://cdn.nlark.com/yuque/__latex/6865f01049936504d903dd8bf6ba1ae8.svg#card=math&code=x%5E0&id=Rw38M)是离散的确定值，不希望生成出训练集的图片？
10. 重写q为后验分布（因为markov分布与历史状态无关，这里条件概率可以先多写一个![](https://cdn.nlark.com/yuque/__latex/6865f01049936504d903dd8bf6ba1ae8.svg#card=math&code=x%5E0&id=X3JFu)，让![](https://cdn.nlark.com/yuque/__latex/1fc374d71b27df6791ae8cbb3ed9ec63.svg#card=math&code=q%28x%5E%7Bt-1%7D%5Cmid%20x%5Et%29&id=QT6x9)变得可以计算，这可能也是上一步要把第一项去掉的原因，不然0的位置后验不容易算）：

![](https://cdn.nlark.com/yuque/__latex/da5017fafffc631df1282f7c74742096.svg#card=math&code=%5Cbegin%7Baligned%7D%0AK%26%3D%0A%5Csum_%7Bt%3D2%7D%5ET%5Cint-%5Clog%20%5Cfrac%7Bp%28x%5E%7Bt-1%7D%5Cmid%20x%5Et%29%7D%7Bq%28x%5E%7Bt%7D%5Cmid%20x%5E%7Bt-1%7D%2Cx%5E0%29%7D%20q%28x%5E%7B0%3AT%7D%29%20dx%5E%7B0%3AT%7D%2BH_p%28X%5ET%29%5C%5C%0A%26%3D%0A%5Csum_%7Bt%3D2%7D%5ET%5Cint-%5Clog%20%5Cfrac%7Bp%28x%5E%7Bt-1%7D%5Cmid%20x%5Et%29%7D%7Bq%28x%5E%7Bt-1%7D%5Cmid%20x%5E%7Bt%7D%2Cx%5E0%29%7D%5Cfrac%7Bq%28x%5E%7Bt-1%7D%5Cmid%20x%5E0%29%7D%7Bq%28x%5Et%5Cmid%20x%5E0%29%7D%20q%28x%5E%7B0%3AT%7D%29%20dx%5E%7B0%3AT%7D%2BH_p%28X%5ET%29%5C%5C%0A%26%3D%0A%5Csum_%7Bt%3D2%7D%5ET%5Cint-%5Clog%20%5Cfrac%7Bp%28x%5E%7Bt-1%7D%5Cmid%20x%5Et%29%7D%7Bq%28x%5E%7Bt-1%7D%5Cmid%20x%5E%7Bt%7D%2Cx%5E0%29%7Dq%28x%5E%7B0%3AT%7D%29%20dx%5E%7B0%3AT%7D%20%2B%20%5Csum_%7Bt%3D2%7D%5E%7BT%7D%5Cleft%28H_q%28X%5E%7Bt-1%7D%5Cmid%20X%5E0%29%20-%20H_q%28X%5E%7Bt%7D%5Cmid%20X%5E0%29%5Cright%29%20%2B%20H_p%28X%5ET%29%5C%5C%0A%26%3D%0A%5Csum_%7Bt%3D2%7D%5ET%5Cint-%5Clog%20%5Cfrac%7Bp%28x%5E%7Bt-1%7D%5Cmid%20x%5Et%29%7D%7Bq%28x%5E%7Bt-1%7D%5Cmid%20x%5E%7Bt%7D%2Cx%5E0%29%7Dq%28x%5E%7B0%3AT%7D%29%20dx%5E%7B0%3AT%7D%20%2B%20H_q%28X%5E1%5Cmid%20X%5E0%29%20-%20H_q%28X%5ET%5Cmid%20X%5E0%29%20%2B%20H_p%28X%5ET%29%5C%5C%0A%26%3D%0A%5Csum_%7Bt%3D2%7D%5ET%5Cint%20D_%7BKL%7D%28q%28x%5E%7Bt-1%7D%5Cmid%20x%5E%7Bt%7D%2Cx%5E0%29%5Cparallel%20p%28x%5E%7Bt-1%7D%5Cmid%20x%5Et%29%29q%28x%5E0%2Cx%5Et%29%20dx%5E0dx%5Et%20%2B%20H_q%28X%5E1%5Cmid%20X%5E0%29%20-%20H_q%28X%5ET%5Cmid%20X%5E0%29%20%2B%20H_p%28X%5ET%29%0A%5Cend%7Baligned%7D&id=BhVMV)

11. ![](https://cdn.nlark.com/yuque/__latex/65ab8e5bfbba1d104086d7970722ee6c.svg#card=math&code=%5Cbeta_t&id=Fcdf6)也是可学习的（![](https://cdn.nlark.com/yuque/__latex/18d21d6e50fea8f85b241bfeab7de4c7.svg#card=math&code=%5Cbeta_1&id=BzICZ)固定为小常量避免过拟合），因此两个交叉熵项并非常量。

**数学过程中的近似**

- step数有限，最终的![](https://cdn.nlark.com/yuque/__latex/5b686eda6a645d15a1f5db0dd81a6d8a.svg#card=math&code=x%5ET&id=a5z9Q)并非严格的正态分布
- 实操中![](https://cdn.nlark.com/yuque/__latex/65ab8e5bfbba1d104086d7970722ee6c.svg#card=math&code=%5Cbeta_t&id=hAp8f)并非趋于0，带来了两个后果
   - 反向过程![](https://cdn.nlark.com/yuque/__latex/e20eda67afeb6b0ff4d8b0859cdef8d1.svg#card=math&code=p%28x%5E%7Bt-1%7D%5Cmid%20x%5Et%29&id=XhL3N)并非正态分布
   - 通过轨迹求出的![](https://cdn.nlark.com/yuque/__latex/23c94bad78433324681ffe7e419f5405.svg#card=math&code=p%28x%5E0%29&id=qWPBN)并非对轨迹积分的准确估计
   - Jensen不等式的近似（趋于0时，这个不等式会变成等式）
- MLP拟合正态分布的均值和方差不精确（MLP不一定是合适的结构）
- 对![](https://cdn.nlark.com/yuque/__latex/6865f01049936504d903dd8bf6ba1ae8.svg#card=math&code=x%5E0&id=PC2Ny)的特殊处理：![](https://cdn.nlark.com/yuque/__latex/16392572f9fbf2b14941f8b513ce252e.svg#card=math&code=p%28x%5E0%5Cmid%20x%5E1%29%3Dq%28x%5E1%5Cmid%20x%5E0%29&id=Rhk6q)
<a name="BORl6"></a>
### 分布的相乘
生成过程中，为了受控的生成，我们需要给模型生成的每个中间状态的分布![](https://cdn.nlark.com/yuque/__latex/f5b434bf8f4f16354a95193bf460d69c.svg#card=math&code=q%28x%5Et%29&id=NMu5Y)乘一个函数![](https://cdn.nlark.com/yuque/__latex/873c71819a205d422a5ede6d42569070.svg#card=math&code=r%28x%5Et%29&id=Mu6EV)，得到修改后的分布![](https://cdn.nlark.com/yuque/__latex/6aae1646f2113b13eaae7a929d2db73b.svg#card=math&code=%5Ctilde%7Bq%7D%28x%5Et%29&id=lzLyk)，用![](https://cdn.nlark.com/yuque/__latex/46efd5850c745b4545620c2e3ae59918.svg#card=math&code=%5Ctilde%7BZ%7D_t&id=LLUDS)表示归一化函数：定义![](https://cdn.nlark.com/yuque/__latex/e3c12cee6f40c4e39badd57985e9f3e5.svg#card=math&code=%5Ctilde%7Bq%7D%28x%5E0%29&id=ni035)与![](https://cdn.nlark.com/yuque/__latex/25f846258e6653f7f40a02ba40491b1f.svg#card=math&code=%5Ctilde%7Bq%7D%28x%5E%7Bt%2B1%7D%5Cmid%20x%5Et%29&id=cOgsv)后，以下三个公式均可从贝叶斯公式推出

- ![](https://cdn.nlark.com/yuque/__latex/263672e48f6456769e618de7b096aeae.svg#card=math&code=%5Ctilde%7Bq%7D%28x%5Et%29%3D%5Cfrac%7B1%7D%7B%5Ctilde%7BZ%7D_t%7Dq%28x%5Et%29r%28x%5Et%29&id=uVlU0)
- ![](https://cdn.nlark.com/yuque/__latex/d5ff7556cccfd62a8a1e0d3fd776a8c8.svg#card=math&code=%5Ctilde%7Bq%7D%28x%5E%7Bt%2B1%7D%5Cmid%20x%5Et%29%5Cpropto%20q%28x%5E%7Bt%2B1%7D%5Cmid%20x%5Et%29r%28x%5E%7Bt%2B1%7D%29&id=mJRmy)
- ![](https://cdn.nlark.com/yuque/__latex/8796b36f993ed8a918d967087c394ca6.svg#card=math&code=%5Ctilde%7Bq%7D%28x%5Et%20%5Cmid%20x%5E%7Bt%2B1%7D%29%5Cpropto%20q%28x%5Et%20%5Cmid%20x%5E%7Bt%2B1%7D%29r%28x%5Et%29&id=xPvtU)

![image.png](https://cdn.nlark.com/yuque/0/2023/png/2787610/1679625453096-cad0d1fc-1338-465b-a4c9-300ba2f67b03.png#averageHue=%23a6a6a6&clientId=u4b427259-06cb-4&from=paste&height=255&id=u9c722960&name=image.png&originHeight=391&originWidth=403&originalType=binary&ratio=1&rotation=0&showTitle=false&size=168703&status=done&style=none&taskId=u6b2bcdb1-a755-4429-a595-48242493d0c&title=&width=262.3999938964844)<br />因此，待拟合的分布p也被类似的修改：![](https://cdn.nlark.com/yuque/__latex/9786e807c1805f1c92e2fa773a6027f2.svg#card=math&code=%5Ctilde%7Bp%7D%28x%5Et%20%5Cmid%20x%5E%7Bt%2B1%7D%29%5Cpropto%20p%28x%5Et%20%5Cmid%20x%5E%7Bt%2B1%7D%29r%28x%5Et%29&id=qPrVb)。实操中比如从上图恢复中间的噪声，就会在学出来的反向过程中，每一步都对已知的图乘以一个delta函数来让它确定，对未知的图乘一个常数。
<a name="VtEVx"></a>
### 反向过程的熵
![image.png](https://cdn.nlark.com/yuque/0/2023/png/2787610/1679714169013-c37503db-2fc0-4caf-9e50-b6e25b39306c.png#averageHue=%23faf8f6&clientId=uf63c33e3-3609-4&from=paste&height=85&id=u0cb0531b&name=image.png&originHeight=126&originWidth=725&originalType=binary&ratio=1&rotation=0&showTitle=false&size=20696&status=done&style=none&taskId=udd5c2d3f-b179-47b2-aa56-480169c54bf&title=&width=487)

- 小于等于前向过程的熵
- 大于等于前向过程的熵减去后前两step熵的增量
<a name="IXQ0P"></a>
### 一些疑问

- 什么是edge effect：看new bing的回复，应该是表示![](https://cdn.nlark.com/yuque/__latex/804445b08932596dcb951a19899de4bc.svg#card=math&code=p%28x_0%5Cmid%20x_1%29&id=UhVce)再往前没有数据了，不同于中间的条件概率，导致其表现与其他step的条件概率不同。
<a name="Sey5H"></a>
## 生成高清图片
> [Denoising diffusion probabilistic models](https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html)

<a name="Wwfe0"></a>
### 前向过程的快捷计算
利用重参数化技巧，可以计算任何一个step的图像：![image.png](https://cdn.nlark.com/yuque/0/2023/png/2787610/1679716063541-cf1d8e31-10d3-4e65-8f09-4cea95343591.png#averageHue=%23f7f5f4&clientId=uf63c33e3-3609-4&from=paste&height=30&id=u2df95de8&name=image.png&originHeight=62&originWidth=605&originalType=binary&ratio=1&rotation=0&showTitle=false&size=8965&status=done&style=none&taskId=ud1df2986-9087-46db-b701-a44906266ac&title=&width=290)![image.png](https://cdn.nlark.com/yuque/0/2023/png/2787610/1679716109882-1a3bbbb0-64c8-4114-96cc-fa8bb54d622b.png#averageHue=%23f6f4f2&clientId=uf63c33e3-3609-4&from=paste&height=26&id=u0499b10d&name=image.png&originHeight=53&originWidth=509&originalType=binary&ratio=1&rotation=0&showTitle=false&size=6540&status=done&style=none&taskId=u5a1f38ac-d1fb-4ca7-aaaa-3578ea3743e&title=&width=245.20001220703125)

1. ![](https://cdn.nlark.com/yuque/__latex/14de95220d8adc84bde9694522cbc18c.svg#card=math&code=x_2%20%3D%5Csqrt%7B%5Calpha_2%7Dx_1%2B%5Csqrt%7B1-%5Calpha_2%7D%5Cepsilon_1%0A%3D%5Csqrt%7B%5Calpha_1%5Calpha_2%7Dx_0%2B%5Csqrt%7B%5Calpha_2%281-%5Calpha_1%29%7D%5Cepsilon_0%2B%5Csqrt%7B1-%5Calpha_2%7D%5Cepsilon_1&id=bwlD8)
2. 为了求方差项的方差，对它做平方，得到![](https://cdn.nlark.com/yuque/__latex/36776a18ea087c9931b41b0c89af5bad.svg#card=math&code=E%5B%5Calpha_2%5Cepsilon_0%5E2-%5Calpha_1%5Calpha_2%5Cepsilon_0%5E2%2B%281-%5Calpha_2%29%5Cepsilon_1%5E2%5D%3D1-%5Calpha_1%5Calpha_2&id=GvNDM)
<a name="NPvwB"></a>
### 优化目标的快速计算
经过文章推导，优化目标（ICML-2015中的![](https://cdn.nlark.com/yuque/__latex/23388e6d492bf4dc3c4df3aa3049e851.svg#card=math&code=K%3D%5Csum_%7Bt%3D1%7D%5ET%5Cint-%5Clog%20%5Cfrac%7Bp%28x%5E%7Bt-1%7D%5Cmid%20x%5Et%29%7D%7Bq%28x%5E%7Bt%7D%5Cmid%20x%5E%7Bt-1%7D%29%7D%20q%28x%5E%7B0%3AT%7D%29%20dx%5E%7B0%3AT%7D%2BH_p%28X%5ET%29&id=thL5n)）可以写成如下形式<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/2787610/1679716304525-5544a77b-83db-4603-9c73-5dc392cac0e3.png#averageHue=%23f2f1f0&clientId=uf63c33e3-3609-4&from=paste&height=66&id=udd40511e&name=image.png&originHeight=128&originWidth=1400&originalType=binary&ratio=1&rotation=0&showTitle=false&size=22398&status=done&style=none&taskId=ue17d9673-9d4c-41a4-adff-948731fdfb2&title=&width=724.4000244140625)<br />这个形式比原始论文的更清晰，并且没有在这一步就把t=1（![](https://cdn.nlark.com/yuque/__latex/f0b3146aa84eeda704e70d3a1d7c3cde.svg#card=math&code=L_0&id=C90DH)）给去掉。<br />后验概率q可以如下计算：<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/2787610/1679719968051-18ca88ef-a0f8-4c0b-9172-849949005223.png#averageHue=%23f9f8f7&clientId=uf63c33e3-3609-4&from=paste&height=99&id=uab73ae6b&name=image.png&originHeight=161&originWidth=1297&originalType=binary&ratio=1&rotation=0&showTitle=false&size=30884&status=done&style=none&taskId=u2078718e-1e77-45c3-8f44-c1804ff232f&title=&width=798.5999755859375)<br />因此上面的所有KL散度，都是**高斯分布**间的，不需要蒙特卡洛方法，只需要Rao-Blackwellized方法计算，有close-formed公式
<a name="IiwHV"></a>
### 学习算法

- 本文固定![](https://cdn.nlark.com/yuque/__latex/65ab8e5bfbba1d104086d7970722ee6c.svg#card=math&code=%5Cbeta_t&id=hemz9)，因此![](https://cdn.nlark.com/yuque/__latex/3a190ff1f2a45ed46e756587cac6d7dd.svg#card=math&code=L_T&id=b9vq8)变成一个常数，无需考虑
- 方差固定为![](https://cdn.nlark.com/yuque/__latex/46c9804f573362e1f016b51d18e409fd.svg#card=math&code=%5Csigma%5E2_t%20I&id=dO6bM)，![](https://cdn.nlark.com/yuque/__latex/e61b4f9742690cd011ca3c75d22b4584.svg#card=math&code=%5Csigma_t%5E2%3D%5Cbeta_t&id=BvGJ7)，不学习
- 给定上述方差后，关于![](https://cdn.nlark.com/yuque/__latex/177eb18536b3d0d3edae6832d586fb27.svg#card=math&code=L_%7Bt-1%7D&id=BM0Pk)可以用如下公式计算：
   1. ![image.png](https://cdn.nlark.com/yuque/0/2023/png/2787610/1679722716115-9db44f11-afb9-450c-97a5-84275d516c2b.png#averageHue=%23fbf9f8&clientId=u09bcef6a-b0b1-4&from=paste&height=69&id=u360e2f12&name=image.png&originHeight=90&originWidth=602&originalType=binary&ratio=1&rotation=0&showTitle=false&size=10999&status=done&style=none&taskId=u42eecabd-ddbb-432a-8a3b-a30bf2ff807&title=&width=460.6000061035156)
   2. 更进一步，![](https://cdn.nlark.com/yuque/__latex/89bf5a1957a0bfae51a0f915e1f68654.svg#card=math&code=x_t%28x_0%2C%5Cepsilon%29%3D%5Csqrt%7B%5Cbar%7B%5Calpha%7D_t%7Dx_0%2B%5Csqrt%7B1-%5Cbar%7B%5Calpha%7D_t%7D%5Cepsilon&id=vcpX7)，带入得到

![image.png](https://cdn.nlark.com/yuque/0/2023/png/2787610/1679728054391-32d4da0e-1bee-4596-bbeb-e6069adbbc0c.png#averageHue=%23fbfaf9&clientId=u4684041f-d240-4&from=paste&height=204&id=ud5140d45&name=image.png&originHeight=302&originWidth=1452&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=48548&status=done&style=none&taskId=u722e7d3b-baa8-4a3c-98ae-07d39b8c662&title=&width=982.4000244140625)

- ![](https://cdn.nlark.com/yuque/__latex/4151fae1392710b9db51464c8104f646.svg#card=math&code=%5Cmu_%5Ctheta&id=iphGR)应该学习的是![image.png](https://cdn.nlark.com/yuque/0/2023/png/2787610/1679728266870-63e9f1e3-75f4-4f43-9d29-a276ea1a67f2.png#averageHue=%23f7f6f5&clientId=u4684041f-d240-4&from=paste&height=59&id=u8a04f34c&name=image.png&originHeight=90&originWidth=411&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=14207&status=done&style=none&taskId=u64c9e6d4-fea6-4cda-b026-bf907acee20&title=&width=267.8000183105469)，
   - 从而我们拆解![image.png](https://cdn.nlark.com/yuque/0/2023/png/2787610/1679728386334-d8de5d3c-6942-49fc-b70d-1e805bbad839.png#averageHue=%23f9f7f7&clientId=u4684041f-d240-4&from=paste&height=65&id=u25f6b9e6&name=image.png&originHeight=111&originWidth=1321&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=20286&status=done&style=none&taskId=u26a3295b-6dd3-4a9a-bcc8-78be7940e8c&title=&width=776.4000244140625)
   - 生成阶段，采用公式![image.png](https://cdn.nlark.com/yuque/0/2023/png/2787610/1679728871856-47df41c6-e674-428d-9ac8-e4f6c09a855f.png#averageHue=%23f7f5f4&clientId=u4684041f-d240-4&from=paste&height=57&id=u12f50400&name=image.png&originHeight=77&originWidth=968&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=17292&status=done&style=none&taskId=u43a40c93-be56-4a32-96e3-724477ee250&title=&width=710.4000244140625)

![image.png](https://cdn.nlark.com/yuque/0/2023/png/2787610/1679728626577-66ee24a8-5c96-4c15-9ac7-3aac1a862f14.png#averageHue=%23f3f2f2&clientId=u4684041f-d240-4&from=paste&height=241&id=u01bb0181&name=image.png&originHeight=377&originWidth=1526&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=90703&status=done&style=none&taskId=u9a06a69d-8260-4fd0-bbb2-6f6e1dbce5b&title=&width=976.4000244140625)<br />**前面所有复杂的数学公式，得到的loss可以简单定义为，实际噪声与预测噪声的平方差**<br />**前面公式的作用在于生成阶段**![](https://cdn.nlark.com/yuque/__latex/6d8baf6861f4df36eba52b385984c38c.svg#card=math&code=x_%7Bt-1%7D&id=W0gpa)**的计算**<br />此外，前面推导中不同step的weight应该是不同的：![image.png](https://cdn.nlark.com/yuque/0/2023/png/2787610/1679896087102-b19248b8-a1d7-4574-9b63-b41e8ef9b01c.png#averageHue=%23f8f7f6&clientId=u4684041f-d240-4&from=paste&height=43&id=u8ee0c36e&name=image.png&originHeight=104&originWidth=903&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=18489&status=done&style=none&taskId=u26ef52fb-6110-4569-bdf4-3c2632ed723&title=&width=372.4000244140625)，这里把这个weight去掉了实际相当于强调了与理论bound不同的某些step（对小的t取了偏小的weight），这个side-effect实际更有利于模型训练，因为对更大噪声的去噪更困难，需要网络更多去考虑。
<a name="L7EZ9"></a>
### 最后一步![](https://cdn.nlark.com/yuque/__latex/804445b08932596dcb951a19899de4bc.svg#card=math&code=p%28x_0%5Cmid%20x_1%29&id=wZ4IN)的处理
图像数据被从[0,255]normalize到[-1,1]，为了确定discrete的像素值，文章会对最后学习到的![](https://cdn.nlark.com/yuque/__latex/804445b08932596dcb951a19899de4bc.svg#card=math&code=p%28x_0%5Cmid%20x_1%29&id=AJYuz)的连续条件分布取一个像素点对应区间的积分，得到离散的像素值（i表示数据的下标，即channel、h、w）：<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/2787610/1679895075823-9d4f4606-b43a-40bf-bef3-463fbb6ab75d.png#averageHue=%23fbfaf9&clientId=u4684041f-d240-4&from=paste&height=165&id=u61c11829&name=image.png&originHeight=250&originWidth=1103&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=35405&status=done&style=none&taskId=uc75eccaa-c000-4896-a401-30b2422b852&title=&width=726.4000244140625)<br />注：与其他step的采样的区别是，最后这一步对方差的处理没有随机性，而是noiseless的（直接取确定的积分）
<a name="UVNSi"></a>
### 其他细节（实验）
![](https://cdn.nlark.com/yuque/__latex/65ab8e5bfbba1d104086d7970722ee6c.svg#card=math&code=%5Cbeta_t&id=AUPly)：从![](https://cdn.nlark.com/yuque/__latex/0862c3e735b7923c475f6e3c30ff4961.svg#card=math&code=10%5E%7B-4%7D&id=ckhWX)线性增长到![](https://cdn.nlark.com/yuque/__latex/ed89dd9c00be8dd962314490330e6f1e.svg#card=math&code=0.02&id=jLqFS)，文章解释这样会尽可能减小![](https://cdn.nlark.com/yuque/__latex/3a190ff1f2a45ed46e756587cac6d7dd.svg#card=math&code=L_T&id=HaMaR)，即前向结果与正态分布的一致性。<br />backbone：U-Net，不同t的参数共享，t以Transformer sinusoidal position embedding的形式喂入网络。<br />**Ablations**<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/2787610/1679898439544-a4954de7-63be-4ed1-99ee-60247c46b607.png#averageHue=%23f0eeed&clientId=u4684041f-d240-4&from=paste&height=219&id=u2cc05bea&name=image.png&originHeight=304&originWidth=486&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=42919&status=done&style=none&taskId=uf98ff680-c681-485f-aff4-b3d50a1f42f&title=&width=349.8000183105469)

- 模型直接predict![](https://cdn.nlark.com/yuque/__latex/756a643380ff53c0692dbc2e7e930a35.svg#card=math&code=%5Cmu&id=sF4u1)而非![](https://cdn.nlark.com/yuque/__latex/2dd779ab5086139fab0042676d5d851d.svg#card=math&code=%5Cepsilon%0A&id=D1MEJ)：不能使用简化了的L，必须使用原始的weighted form，否则效果很差
- 模型使用学习出来的方差矩阵（对角阵）：训练不稳定

**通过共享不同step的图片，可以生成不同相似度的图：**<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/2787610/1679899492835-f5463b09-399b-41ab-8c82-c9eb90a4825e.png#averageHue=%237f7660&clientId=u4684041f-d240-4&from=paste&height=278&id=uf948e24d&name=image.png&originHeight=348&originWidth=1246&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=755329&status=done&style=none&taskId=uf3a42622-9cd6-4cde-a6ff-049d402b495&title=&width=996.8)<br />**Diffusion模型适合于图像插值：**<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/2787610/1679899834706-5b7fa77c-ff54-4e04-8a7f-6682ecf1613b.png#averageHue=%23c0b2a9&clientId=u4684041f-d240-4&from=paste&height=182&id=u1fb68210&name=image.png&originHeight=356&originWidth=1530&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=611802&status=done&style=none&taskId=ud7dff6ff-4f47-4115-b74e-8e4da8d0239&title=&width=782.4000244140625)

